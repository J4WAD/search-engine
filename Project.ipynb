{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('stopwords.txt', \"r\", encoding=\"utf-8-sig\")\n",
    "STwords = [word.rstrip() for word in f.readlines()]\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = ET.parse('trec.sample.xml')\n",
    "root = tree.getroot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create inv index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_invindex(file):\n",
    "    \n",
    "    tree = ET.parse(file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    inv_index = defaultdict(list)\n",
    "    docs = []\n",
    "\n",
    "    for idx,child in enumerate(root.findall('.//DOC')):#iterate through all docs (DOC) each containing HEADLINE DOC TEXT\n",
    "        headline = child.find('.//HEADLINE').text #get the headline of the nth doc \n",
    "        docnum = child.find('.//DOCNO').text #get the docno of the nth doc \n",
    "        text = child.find('.//TEXT').text #get the text of the nth doc \n",
    "        combined = headline.strip() + \" \" + text #combination of text+headline\n",
    "        tokenized = re.findall(r'\\d+\\.\\d+(?:bn|m)?|\\w+', combined)#applied tokenization using regex on the combined text+headline\n",
    "        docs.append(docnum)# get the ids \n",
    "        tokenized = [stemmer.stem(word) for word in tokenized if word.lower() not in STwords]#applied stopwords + stemming on tokenized \n",
    "        for pos, word in enumerate(tokenized):\n",
    "            if word in inv_index: #check if the word is in the inverted index first\n",
    "                if docnum in inv_index[word][1]:#check if the document is in the inverted index of the word\n",
    "                    inv_index[word][1][docnum].append(pos) # If the the word already ocurred in the docnum, append the position of the new occurrence.\n",
    "                else:\n",
    "                    inv_index[word][1][docnum] = [pos] # Else, leave position list untouched.\n",
    "\n",
    "            else:#if it is not in the inv index then add it\n",
    "                inv_index[word].append('') # If word is not in inv_index, create key and set frequency to 1.\n",
    "                inv_index[word].append({}) # Declare posting list for new word.   \n",
    "                inv_index[word][1][docnum] = [pos] # For each word, add document ID : [pos] pair.\n",
    "\n",
    "            inv_index[word][0] = len(inv_index[word][1].keys()) # If the word already exists in the inv index, increase the frequency of word.\n",
    "            #inv_index[word][0] -> frequency part >8\n",
    "            #inv_index[word][1] -> posting list\n",
    "    return inv_index, docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_index, docs = create_invindex('trec.sample.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n'jet': [8,\\n              {\\n               '1': [5, 22, 39, 58, 84, 103],\\n               '130': [104, 147, 233],\\n               '138': [563],\\n               '188': [34],\\n               '3556': [12],\\n               '3726': [40, 49, 58, 93, 380, 442, 523, 945],\\n               '3808': [63],\\n               '3923': [97, 326]\\n               }\\n        ],\\n\""
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inv_index[\"tax\"]\n",
    "'''\n",
    "'jet': [8,\n",
    "              {\n",
    "               '1': [5, 22, 39, 58, 84, 103],\n",
    "               '130': [104, 147, 233],\n",
    "               '138': [563],\n",
    "               '188': [34],\n",
    "               '3556': [12],\n",
    "               '3726': [40, 49, 58, 93, 380, 442, 523, 945],\n",
    "               '3808': [63],\n",
    "               '3923': [97, 326]\n",
    "               }\n",
    "        ],\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boolean search parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8,\n",
       " {'1': [5, 22, 39, 58, 84, 103],\n",
       "  '130': [104, 147, 233],\n",
       "  '138': [563],\n",
       "  '188': [34],\n",
       "  '3556': [12],\n",
       "  '3726': [40, 49, 58, 93, 380, 442, 523, 945],\n",
       "  '3808': [63],\n",
       "  '3923': [97, 326]}]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_word(inv_index,word):\n",
    "    if word in inv_index.keys():\n",
    "        return inv_index[word]\n",
    "    else:\n",
    "        return []\n",
    "get_word(inv_index,'jet')\n",
    "# . keys()> docs .values() positions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[65,\n",
       " 92,\n",
       " 163,\n",
       " 282,\n",
       " 361,\n",
       " 3387,\n",
       " 3405,\n",
       " 3441,\n",
       " 3449,\n",
       " 3490,\n",
       " 3533,\n",
       " 3562,\n",
       " 3599,\n",
       " 3699,\n",
       " 3706,\n",
       " 3708,\n",
       " 3734,\n",
       " 3817,\n",
       " 3818]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def proximity_search(inv_index,word1,word2,distance=1):\n",
    "    #a = inv_index[word1][1]\n",
    "    a = get_word(inv_index,word1)[1]\n",
    "    #b = inv_index[word2][1]\n",
    "    b = get_word(inv_index,word2)[1]\n",
    "    match = a.keys() & b.keys()\n",
    "    newdict = {k:(a[k],b[k]) for k in match}# temp dictionary to store {documentID: ([positions of word a], [positions of word b])},\n",
    "    documents = set()\n",
    "    for key,val in newdict.items():\n",
    "        for i in range(len(val)-1):#i is always 0 that works on list of words A\n",
    "            for j in range(len(val[i])):# j works on each element of list of words A (length of list A)\n",
    "                temp = val[i][j]\n",
    "                for k in range(len(val[i+1])):# k works on each element of list of words B(length of list B)\n",
    "                    temp2 = val[i+1][k]\n",
    "                    if distance == 1:\n",
    "                        if temp2 - temp == distance:\n",
    "                            documents.add(key)\n",
    "                    else:\n",
    "                        if abs(temp2 - temp) <= distance:\n",
    "                            documents.add(key)\n",
    "                        \n",
    "\n",
    "    documents = sorted(list(map(int,documents)))\n",
    "    return documents\n",
    "\n",
    "proximity_search(inv_index, 'incom', 'tax', 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_parser(query,inv_index):\n",
    "    print(query)\n",
    "    query = query.lower()#lowercase query\n",
    "    \n",
    "    if query.find(\" \") == -1 and query.find('#') == -1: #if query has no spaces and no #, means we only working on one word, then return the document ids\n",
    "        word = stemmer.stem(query)\n",
    "        docs = get_word(inv_index,word)[1].keys()\n",
    "        return sorted(map(int,docs))\n",
    "    \n",
    "    if query.find('\"') != -1:\n",
    "        tokenized_query = re.findall(r'\"(.+?)\"',query)[0]#regex to return the first [0] phrase queryprocessed \"your query here\"\n",
    "        tokenized_query = tokenized_query.split()# apply tokenization\n",
    "        tokenized_query = [stemmer.stem(word) for word in tokenized_query]#apply stemming\n",
    "        temp1 = tokenized_query #temp1 contains two words a and b\n",
    "        result = proximity_search(inv_index,tokenized_query[0],tokenized_query[1]) #proximity search with distance=1 means phrase search.\n",
    "        if query.find(' and ') != -1:\n",
    "            if query.find('not') != -1:\n",
    "                if query.count('\"') == 4:\n",
    "                    tokenized_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                    tokenized_query = tokenized_query.split()\n",
    "                    tokenized_query = [stemmer.stem(word) for word in tokenized_query]\n",
    "                    notword = re.findall(r'not\\s\"(.+?)\"',query)[0].split()\n",
    "                    notword = [stemmer.stem(word) for word in notword]\n",
    "                    if notword == temp1:\n",
    "                        temp = proximity_search(inv_index,tokenized_query[0],tokenized_query[1])\n",
    "                        return sorted(set(temp).difference(set(result)))\n",
    "                    else:\n",
    "                        temp = proximity_search(inv_index,tokenized_query[0],tokenized_query[1])\n",
    "                        return sorted(set(result).difference(set(temp)))\n",
    "                \n",
    "                notword = re.findall(r'not\\s(.+?)(?:$|\\s)',query)[0]\n",
    "                w = stemmer.stem(re.sub(r'\"(.*)\"|and|not|\\s','',query))\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                if notword.find('\"') != -1:\n",
    "                    return sorted(set(map(int,w)).difference(set(result)))\n",
    "                else:\n",
    "                    return sorted(set(result).difference(set(map(int,w))))\n",
    "            \n",
    "            else:\n",
    "                if query.count('\"') == 4: #if the query contains two phrases\n",
    "                    tokenized_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                    tokenized_query = tokenized_query.split()\n",
    "                    tokenized_query = [stemmer.stem(word) for word in tokenized_query]\n",
    "                    temp = proximity_search(inv_index,tokenized_query[0],tokenized_query[1])\n",
    "                    return sorted(set(map(int,temp)).intersection(set(result)))\n",
    "                    \n",
    "                w = stemmer.stem(re.sub(r'\"(.*)\"|and|\\s','',query))#remove everything except for the word after and\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                return sorted(set(map(int,w)).intersection(set(result)))\n",
    "        \n",
    "        elif query.find(' or ') != -1:\n",
    "            if query.count('\"') == 4:\n",
    "                tokenized_query = re.findall(r'\"(.+?)\"',query)[1]\n",
    "                tokenized_query = tokenized_query.split()\n",
    "                tokenized_query = [stemmer.stem(word) for word in tokenized_query]\n",
    "                temp = proximity_search(inv_index,tokenized_query[0],tokenized_query[1])\n",
    "                return sorted(set(result).union(temp))\n",
    "            else:\n",
    "                w = stemmer.stem(re.sub(r'\"(.*)\"|or|\\s','',query))\n",
    "                w = get_word(inv_index, w)[1].keys()\n",
    "                return sorted(set(result).union(set(map(int,w))))\n",
    "        \n",
    "        else:\n",
    "            return result\n",
    "        \n",
    "    if query.find('#') != -1:\n",
    "        distance = int(re.findall(r'\\d+', query)[0])#get the number in the query > the distance\n",
    "        tokenized_query = re.findall(r'\\((.+)\\)', query)[0]#regex to return anything inside parantheses\n",
    "        tokenized_query = tokenized_query.split(',')#tokenization\n",
    "        tokenized_query = [stemmer.stem(word.strip()) for word in tokenized_query]#stemming\n",
    "        return proximity_search(inv_index,tokenized_query[0],tokenized_query[1],distance=distance)#proximity search with specific distance\n",
    "    \n",
    "    if query.find(' and ') != -1:\n",
    "        if query.find('not') != -1:#incase of and not\n",
    "            notword = re.findall(r'not\\s(.+?)(?:$|\\s)',query)[0]#contains the word \"NOT word\"\n",
    "            w = re.sub(notword, '', query) #remove 'not word' from query and put it in w\n",
    "            w = stemmer.stem(re.sub(r'and|not', '', w).strip())# remove 'and' & 'not'\n",
    "            notword = stemmer.stem(notword)\n",
    "            notword = set(map(int,get_word(inv_index, notword)[1].keys()))\n",
    "            w = set(map(int,get_word(inv_index, w)[1].keys()))\n",
    "            return sorted(w.difference(notword))\n",
    "\n",
    "        else: #in case of just and\n",
    "            w1 = set(map(int,get_word(inv_index,stemmer.stem(re.findall(r'^\\w+',query)[0]))[1].keys()))#w1 contains documents of the word before AND\n",
    "            w2 = set(map(int,get_word(inv_index, stemmer.stem(re.findall(r'and\\s(.+)',query)[0]))[1].keys()))#w2 contains documents of the word after  AND\n",
    "            return sorted(w1.intersection(w2))\n",
    "        \n",
    "    if query.find(' or ') != -1:\n",
    "        w1 = set(map(int,get_word(inv_index,stemmer.stem(re.findall(r'^\\w+',query)[0]))[1].keys()))#w1 contains documents of the word before OR \n",
    "        w2 = set(map(int,get_word(inv_index,stemmer.stem(re.findall(r'or\\s(.+)',query)[0]))[1].keys()))#w2 contains documents of the word after  OR (after stemming)\n",
    "        return sorted(w1.union(w2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Happiness\n",
      "[58, 136, 137, 196, 264, 290, 341, 372, 3329, 3362, 3443, 3474, 3638, 3730, 3773, 3856, 3864, 3913]\n",
      "\n",
      "\n",
      "Edinburgh AND Scotland\n",
      "[351]\n",
      "\n",
      "\n",
      "income OR taxes\n",
      "[3, 14, 16, 23, 24, 32, 33, 34, 39, 41, 42, 43, 46, 65, 92, 106, 112, 113, 125, 141, 144, 146, 161, 163, 168, 170, 171, 172, 173, 175, 177, 178, 184, 185, 186, 190, 216, 226, 228, 241, 248, 257, 262, 268, 270, 282, 287, 304, 313, 314, 315, 316, 317, 319, 322, 325, 326, 328, 329, 351, 357, 360, 361, 366, 370, 375, 3324, 3327, 3329, 3331, 3332, 3333, 3336, 3337, 3341, 3342, 3343, 3345, 3347, 3374, 3387, 3388, 3390, 3396, 3397, 3401, 3403, 3404, 3405, 3407, 3409, 3412, 3415, 3417, 3420, 3437, 3439, 3441, 3443, 3444, 3449, 3460, 3469, 3470, 3472, 3473, 3474, 3475, 3480, 3481, 3482, 3490, 3494, 3495, 3498, 3503, 3504, 3511, 3519, 3525, 3527, 3528, 3529, 3530, 3531, 3532, 3533, 3535, 3537, 3542, 3543, 3548, 3561, 3562, 3563, 3567, 3569, 3580, 3582, 3586, 3589, 3590, 3591, 3592, 3593, 3595, 3596, 3597, 3599, 3602, 3604, 3606, 3607, 3608, 3610, 3611, 3612, 3613, 3617, 3618, 3622, 3625, 3628, 3629, 3630, 3636, 3653, 3654, 3658, 3661, 3666, 3669, 3672, 3673, 3680, 3692, 3693, 3694, 3699, 3700, 3701, 3704, 3705, 3706, 3707, 3708, 3709, 3710, 3714, 3715, 3726, 3734, 3760, 3771, 3775, 3781, 3788, 3790, 3792, 3793, 3795, 3805, 3812, 3816, 3817, 3818, 3820, 3821, 3822, 3824, 3826, 3827, 3828, 3829, 3830, 3831, 3834, 3835, 3838, 3841, 3842, 3846, 3848, 3849, 3865, 3869, 3876, 3890, 3901, 3913, 3928, 3929, 3932, 3937, 3939]\n",
      "\n",
      "\n",
      "\"income taxes\"\n",
      "[65, 92, 282, 361, 3405, 3441, 3449, 3562, 3706, 3708, 3734, 3818]\n",
      "\n",
      "\n",
      "#20(income, taxes)\n",
      "[65, 92, 163, 282, 361, 3387, 3405, 3441, 3449, 3490, 3533, 3562, 3589, 3590, 3599, 3606, 3617, 3699, 3705, 3706, 3708, 3734, 3816, 3817, 3818, 3829, 3834, 3835]\n",
      "\n",
      "\n",
      "\"middle east\" AND peace\n",
      "[219, 223, 288, 305, 3549, 3663, 3762, 3766]\n",
      "\n",
      "\n",
      "\"islam religion\"\n",
      "[]\n",
      "\n",
      "\n",
      "\"Financial times\" AND NOT BBC\n",
      "[38, 55, 133, 140, 146, 306, 347, 3580, 3690, 3692, 3740, 3907]\n",
      "\n",
      "\n",
      "#15(dow,stocks)\n",
      "[14, 154]\n"
     ]
    }
   ],
   "source": [
    "print(query_parser('Happiness',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('Edinburgh AND Scotland',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('income OR taxes',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"income taxes\"',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('#20(income, taxes)',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"middle east\" AND peace',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"islam religion\"',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('\"Financial times\" AND NOT BBC',inv_index))\n",
    "print('\\n')\n",
    "print(query_parser('#15(dow,stocks)',inv_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scotland\n",
      "Window\n",
      "replacing\n",
      "condemning\n",
      "income OR taxes\n",
      "income AND NOT taxes\n",
      "\"income taxes\"\n",
      "#10(income, taxes)\n",
      "\"middle east\" AND peace\n"
     ]
    }
   ],
   "source": [
    "def read_queries_boolean(file, inv_index):\n",
    "    with open(file, 'r') as file, open('results.boolean.txt', 'w') as outfile:\n",
    "        for i,line in enumerate(file):\n",
    "            data = line.strip()\n",
    "            results = query_parser(data,inv_index)\n",
    "            for result in results:\n",
    "                outfile.write(str(i+1) + ',' + str(result) + '\\n')\n",
    "                \n",
    "read_queries_boolean('queryprocessed.lab2.txt', inv_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ranked search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create tfidf document index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ranked_search(query, inv_index, docs):\n",
    "    print(query)\n",
    "    queryprocessed = re.findall(r'\\w+',query)#all words in query (Tokenization)\n",
    "    queryprocessed = [stemmer.stem(word) for word in queryprocessed if stemmer.stem(word.lower()) not in STwords]\n",
    "\n",
    "    query_index = {}\n",
    "    tfidf_index = {}\n",
    "    for doc in docs:#for each doc create dict tfidf\n",
    "        tfidf_index[doc] = {} \n",
    "\n",
    "    for word in queryprocessed:\n",
    "        query_index[word] = 1 + math.log(queryprocessed.count(word),10)\n",
    "        for doc in docs:\n",
    "            # Search which documents contain the word and calculate tfidf, if word not in document, then tfidf=0.\n",
    "                if doc in inv_index[word][1].keys():\n",
    "                    tfidf_index[doc][word] = (1 + math.log(len(inv_index[word][1][doc]),10)) * (math.log((len(docs)/inv_index[word][0]),10))# 1+ log(tf [t,d])*log(N/df)\n",
    "                else:\n",
    "                    tfidf_index[doc][word] = 0       #if word not in document, then tfidf=0.\n",
    "                    \n",
    "    scores = []#sum of w(t,d)\n",
    "    for doc in docs:\n",
    "        score = 0\n",
    "        for word in queryprocessed:\n",
    "            if tfidf_index[doc][word] != 0:\n",
    "                score += query_index[word] * (tfidf_index[doc][word])\n",
    "        if score > 0:\n",
    "            scores.append((doc,score))\n",
    "    \n",
    "    return sorted(scores, key=lambda tup: tup[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income tax reduction\n",
      "[('65', 4.804034014375512), ('3533', 4.7264081984633775), ('3562', 3.5453626928666857), ('3608', 3.4909556861709032), ('141', 3.3261651748396766), ('361', 3.3261651748396766), ('92', 3.2310821930752125), ('3829', 3.1818363683925575), ('3420', 3.1273075274176665), ('3734', 3.05610026861608)]\n",
      "\n",
      "\n",
      "stock market in Japan\n",
      "[('3693', 3.7149285503334486), ('3459', 3.4245701312235775), ('287', 3.3848398395205095), ('21', 3.3128376677560882), ('3416', 3.309031248787746), ('3570', 3.177220857140374), ('3797', 3.121412090937466), ('3931', 2.9838837651652943), ('3495', 2.9256011815861958), ('3584', 2.900786909592978)]\n",
      "\n",
      "\n",
      "health industry\n",
      "[('252', 3.053321495111291), ('351', 3.002212881653417), ('3648', 2.7095653718849486), ('129', 2.612484631084815), ('3508', 2.4949732706802545), ('3370', 2.4597902931929645), ('101', 2.3704698314537866), ('3443', 2.330305967012219), ('38', 2.217775493561936), ('216', 2.1691837046408216)]\n",
      "\n",
      "\n",
      "the Robotics industries\n",
      "[('129', 6.504299858263528), ('135', 5.52839017498188), ('133', 4.51524972730621), ('140', 3.4773972357358214), ('138', 3.0622326376567703), ('3871', 2.72814282513163), ('187', 2.6041496239812187), ('3927', 2.6041496239812187), ('3503', 1.2249701415352734), ('3437', 1.1671735598381754)]\n",
      "\n",
      "\n",
      "the peace process in the middle east\n",
      "[('3549', 8.35205499528987), ('288', 8.29033716451255), ('305', 7.848154144956597), ('223', 6.125169579706628), ('3663', 5.5502480663811316), ('188', 4.8932142458960595), ('219', 4.862648528669174), ('3762', 4.862648528669174), ('3766', 3.9924229016956607), ('55', 3.4809592951190726)]\n",
      "\n",
      "\n",
      "information retrieval\n",
      "[('3920', 5.191015575949954), ('3917', 4.20723731576072), ('169', 2.5228787452803374), ('3909', 2.2812407424719465), ('3910', 1.9827996564761647), ('3908', 1.8292298799525566), ('137', 1.684358570480383), ('140', 1.684358570480383), ('253', 1.5882820002296456), ('3745', 1.5882820002296456)]\n",
      "\n",
      "\n",
      "Dow Jones industrial average stocks\n",
      "[('14', 7.457721544026303), ('3773', 6.861256564995538), ('3694', 6.746543571301613), ('3460', 6.439683238963925), ('118', 5.880025374200811), ('331', 5.680521621894936), ('154', 5.410401640602358), ('317', 5.035361411665872), ('3560', 5.035361411665872), ('3692', 5.021595354621844)]\n",
      "\n",
      "\n",
      "will be there a reduction in the income taxes?\n",
      "[('65', 4.804034014375512), ('3533', 4.7264081984633775), ('3562', 3.5453626928666857), ('3608', 3.4909556861709032), ('141', 3.3261651748396766), ('361', 3.3261651748396766), ('92', 3.2310821930752125), ('3829', 3.1818363683925575), ('3420', 3.1273075274176665), ('3734', 3.05610026861608)]\n",
      "\n",
      "\n",
      "the gold prices versus the dollar price\n",
      "[('3694', 5.329981755694031), ('336', 4.953748765199051), ('3877', 4.83372681181214), ('141', 4.750117699620996), ('287', 4.513871455157471), ('3912', 4.44380808057248), ('154', 4.322573419774892), ('226', 4.22663944857049), ('3344', 3.9665762445130497), ('3465', 3.8845156529080667)]\n"
     ]
    }
   ],
   "source": [
    "print(ranked_search(\"income tax reduction\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"stock market in Japan\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"health industry\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the Robotics industries\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the peace process in the middle east\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"information retrieval\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"Dow Jones industrial average stocks\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"will be there a reduction in the income taxes?\", inv_index, docs)[:10])\n",
    "print('\\n')\n",
    "print(ranked_search(\"the gold prices versus the dollar price\", inv_index, docs)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read query and write results to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "income tax reduction\n",
      "peace in the Middle East\n",
      "unemployment rate in UK\n",
      "industry in scotland\n",
      "the industries of computers\n",
      "Microsoft Windows\n",
      "stock market in Japan\n",
      "the education with computers\n",
      "health industry\n",
      "campaigns of political parties\n"
     ]
    }
   ],
   "source": [
    "def queries_ranked(file, tfidf_index, vocab):\n",
    "    res = []\n",
    "    with open(file, 'r') as file, open('results.ranked.txt', 'w') as outfile:\n",
    "        for i,line in enumerate(file):\n",
    "            data = line.strip()\n",
    "            data = re.sub(r'^\\d+\\s','',data).strip()\n",
    "            #ranked_search(query, inv_index, docs)\n",
    "            results = ranked_search(data,tfidf_index, vocab)\n",
    "            for j,result in enumerate(results):\n",
    "                if j < 150:\n",
    "                    outfile.write(str(i+1) + ',' + str(result[0]) + ',' + str(round(result[1],4)) + '\\n')\n",
    "                else:\n",
    "                    continue\n",
    "                        \n",
    "queries_ranked('queryprocessed.txt', inv_index,docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
